{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K0NChUoC0Jdu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def evaluate_absa_predictions(model_file, ground_truth_file):\n",
        "    # Load both Excel files\n",
        "    model_df = pd.read_excel(model_file)\n",
        "    ground_truth_df = pd.read_excel(ground_truth_file)\n",
        "\n",
        "    # List of aspect columns to compare\n",
        "    aspect_cols = [\n",
        "        \"price_sentiment\",\n",
        "        \"delivery time_sentiment\",\n",
        "        \"promotion_sentiment\",\n",
        "        \"review_sentiment\",\n",
        "        \"food condition_sentiment\"\n",
        "    ]\n",
        "\n",
        "    # Check if all columns exist\n",
        "    for col in aspect_cols:\n",
        "        if col not in model_df.columns or col not in ground_truth_df.columns:\n",
        "            raise ValueError(f\"Missing aspect column: {col}\")\n",
        "\n",
        "    total_matches = 0\n",
        "    total_aspects = 0\n",
        "\n",
        "    # Compare each aspect one by one\n",
        "    for col in aspect_cols:\n",
        "        matches = model_df[col].astype(str).str.lower() == ground_truth_df[col].astype(str).str.lower()\n",
        "        total_matches += matches.sum()\n",
        "        total_aspects += len(matches)\n",
        "\n",
        "    total_mismatches = total_aspects - total_matches\n",
        "    accuracy = total_matches / total_aspects if total_aspects > 0 else 0\n",
        "\n",
        "    # Print summary\n",
        "    print(\"Evaluation Summary:\")\n",
        "    print(f\"Total Comparisons: {total_aspects}\")\n",
        "    print(f\"Matches: {total_matches}\")\n",
        "    print(f\"Mismatches: {total_mismatches}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    return {\n",
        "        \"total\": total_aspects,\n",
        "        \"matches\": total_matches,\n",
        "        \"mismatches\": total_mismatches,\n",
        "        \"accuracy\": accuracy\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_absa_predictions(\"absa_ModelResults.xlsx\", \"absa_GroundTruth.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt2t2Ps-0Zeb",
        "outputId": "8a478bdc-b2a9-4cdf-cee3-ab3f698aeace"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Summary:\n",
            "Total Comparisons: 810\n",
            "Matches: 573\n",
            "Mismatches: 237\n",
            "Accuracy: 70.74%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total': 810,\n",
              " 'matches': np.int64(573),\n",
              " 'mismatches': np.int64(237),\n",
              " 'accuracy': np.float64(0.7074074074074074)}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}